import ccxt
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import http.server
import socketserver
import webbrowser
import os
import threading
import time
from datetime import datetime
import itertools

# --- Configuration ---
SYMBOL = 'BTC/USDT'
TIMEFRAME = '1d'
START_DATE = '2018-01-01 00:00:00'
PORT = 8080
SEQ_LENGTH = 30  # 30 lag features

# Explicit Retest Dates (Ground Truth)
RETEST_DATES = [
    '2020-09-06',
    '2021-07-12',
    '2023-09-04',
    '2024-09-02',
    '2025-03-31'
]

def fetch_data():
    print(f"Fetching {SYMBOL} data from Binance starting {START_DATE}...")
    exchange = ccxt.binance()
    since = exchange.parse8601(START_DATE)
    all_ohlcv = []
    
    while True:
        try:
            ohlcv = exchange.fetch_ohlcv(SYMBOL, TIMEFRAME, since=since)
            if not ohlcv:
                break
            all_ohlcv.extend(ohlcv)
            since = ohlcv[-1][0] + 1 
            # Break if we've reached current time (approx)
            if len(ohlcv) < 500: # binance usually returns 500 or 1000
                break
            time.sleep(0.1) # Rate limit politeness
        except Exception as e:
            print(f"Error fetching data: {e}")
            break
            
    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df.set_index('timestamp', inplace=True)
    
    # Filter to ensure uniqueness and order
    df = df[~df.index.duplicated(keep='first')].sort_index()
    return df

def add_features(df):
    print("Engineering features...")
    # Calculate SMAs
    df['sma365'] = df['close'].rolling(window=365).mean()
    df['sma120'] = df['close'].rolling(window=120).mean()
    df['sma40'] = df['close'].rolling(window=40).mean()
    
    # Calculate Distances (Percent)
    # distance = (price - sma) / sma
    df['dist_365'] = (df['close'] - df['sma365']) / df['sma365']
    df['dist_120'] = (df['close'] - df['sma120']) / df['sma120']
    df['dist_40'] = (df['close'] - df['sma40']) / df['sma40']
    
    # Drop NaN values generated by largest SMA (365 days)
    df.dropna(inplace=True)
    return df

def prepare_data(df):
    print("Preparing LSTM sequences...")
    
    # Target Labeling
    df['target'] = 0
    # Mark specific dates as 1. 
    for date_str in RETEST_DATES:
        if date_str in df.index:
            df.loc[date_str, 'target'] = 1
        else:
            print(f"Warning: Retest date {date_str} not found in data (might be a weekend or missing).")

    # Features for LSTM
    feature_cols = ['dist_365', 'dist_120', 'dist_40']
    data = df[feature_cols].values
    targets = df['target'].values
    dates = df.index
    
    # Scaling
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    X, y, prediction_dates = [], [], []
    
    # Create sequences
    # Constraint: "Only use yesterday's data for today's prediction"
    for i in range(SEQ_LENGTH, len(data)):
        # x_seq = data from (i-30) to (i-1)
        x_seq = data_scaled[i-SEQ_LENGTH:i]
        
        X.append(x_seq)
        y.append(targets[i])
        prediction_dates.append(dates[i])
        
    return np.array(X), np.array(y), prediction_dates, df

def build_model(input_shape, params):
    # Unpack params
    units_1 = params.get('units_1', 64)
    units_2 = params.get('units_2', 32)
    dropout = params.get('dropout', 0.2)
    dense_units = params.get('dense_units', 16)
    lr = params.get('learning_rate', 0.001)
    
    model = Sequential([
        Input(shape=input_shape),
        LSTM(units_1, return_sequences=True),
        Dropout(dropout),
        LSTM(units_2),
        Dropout(dropout),
        Dense(dense_units, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])
    return model

def run_grid_search(X, y, class_weight_dict):
    print("\n--- Starting Grid Search ---")
    
    # Define Grid
    param_grid = {
        'units_1': [32, 64],
        'units_2': [16, 32],
        'dropout': [0.2, 0.4],
        'batch_size': [16, 32],
        'epochs': [30] # Keep epochs fixed/lower for search speed, raise for final training if needed
    }
    
    keys, values = zip(*param_grid.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
    
    best_score = -1
    best_params = None
    best_model = None
    
    print(f"Testing {len(combinations)} combinations...")
    
    for i, params in enumerate(combinations):
        print(f"\n[{i+1}/{len(combinations)}] Testing: {params}")
        
        # Build
        model = build_model((X.shape[1], X.shape[2]), params)
        
        # Train (Silent)
        model.fit(
            X, y, 
            epochs=params['epochs'], 
            batch_size=params['batch_size'], 
            class_weight=class_weight_dict,
            verbose=0
        )
        
        # Evaluate
        y_pred_prob = model.predict(X, verbose=0)
        y_pred = (y_pred_prob > 0.5).astype(int)
        
        # Metrics
        f1 = f1_score(y, y_pred)
        prec = precision_score(y, y_pred, zero_division=0)
        rec = recall_score(y, y_pred)
        cm = confusion_matrix(y, y_pred)
        
        # Custom scoring logic: 
        # We prioritize finding AT LEAST some True Positives (TP).
        # Score = F1 score.
        score = f1
        
        tp = cm[1, 1] if cm.shape == (2, 2) else 0
        fp = cm[0, 1] if cm.shape == (2, 2) else 0
        
        print(f"   -> F1: {f1:.4f} | Recall (TP): {tp}/{np.sum(y)} | Precision: {prec:.4f}")
        
        if score > best_score:
            best_score = score
            best_params = params
            best_model = model
            print("   *** New Best Model ***")
            
    print("\n--- Grid Search Complete ---")
    print(f"Best Params: {best_params}")
    print(f"Best F1 Score: {best_score:.4f}")
    return best_model, best_params

def create_plot(df, pred_dates, y_true, y_pred_prob):
    print("Generating mobile-friendly plot...")
    
    # Filter df to match prediction range
    plot_df = df.loc[pred_dates].copy()
    plot_df['prediction_prob'] = y_pred_prob
    plot_df['is_retest'] = y_true
    
    fig = make_subplots(
        rows=2, cols=1, 
        shared_xaxes=True, 
        vertical_spacing=0.05,
        row_heights=[0.7, 0.3],
        subplot_titles=(f"BTC/USDT Price & SMAs", "LSTM Retest Probability")
    )

    # --- Top Panel: Price & SMAs ---
    fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['close'], name='Price', line=dict(color='white', width=1)), row=1, col=1)
    fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['sma365'], name='SMA 365', line=dict(color='orange', width=1.5)), row=1, col=1)
    fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['sma120'], name='SMA 120', line=dict(color='cyan', width=1)), row=1, col=1)
    
    # Mark Actual Retests
    retest_dates_dt = [pd.to_datetime(d) for d in RETEST_DATES if pd.to_datetime(d) in plot_df.index]
    retest_prices = plot_df.loc[retest_dates_dt]['close']
    
    fig.add_trace(go.Scatter(
        x=retest_dates_dt, 
        y=retest_prices, 
        mode='markers', 
        marker=dict(color='yellow', size=12, symbol='star'),
        name='True Retest'
    ), row=1, col=1)

    # --- Bottom Panel: Predictions ---
    # Probability line
    fig.add_trace(go.Scatter(
        x=plot_df.index, 
        y=plot_df['prediction_prob'], 
        name='AI Confidence',
        fill='tozeroy',
        line=dict(color='#00ff00', width=1)
    ), row=2, col=1)
    
    # Threshold line
    fig.add_hline(y=0.5, line_dash="dash", line_color="gray", annotation_text="Decision Threshold", row=2, col=1)

    # Layout for Mobile
    fig.update_layout(
        template='plotly_dark',
        title="LSTM Retest Identifier (Optimized)",
        hovermode='x unified',
        height=800,
        margin=dict(l=10, r=10, t=50, b=10),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    # Write to HTML
    html_content = fig.to_html(full_html=True, include_plotlyjs='cdn')
    
    # Add some custom CSS for better mobile full-screen experience
    custom_style = """
    <style>
        body { margin: 0; padding: 0; background-color: #111; }
        .plotly-graph-div { height: 100vh !important; }
    </style>
    """
    html_content = html_content.replace('</head>', f'{custom_style}</head>')
    
    with open("index.html", "w") as f:
        f.write(html_content)
    print("index.html created.")

def run_server():
    class Handler(http.server.SimpleHTTPRequestHandler):
        def log_message(self, format, *args):
            pass # Silence logs for cleaner console

    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print(f"\nServing results at http://localhost:{PORT}")
        print("Press Ctrl+C to stop.")
        httpd.serve_forever()

def main():
    # 1. Fetch
    df = fetch_data()
    
    # 2. Features
    df = add_features(df)
    
    # 3. Prepare
    X, y, dates, full_df = prepare_data(df)
    
    print(f"Data shape: {X.shape}")
    print(f"Positive samples: {np.sum(y)}")
    
    # 4. Class Weights
    weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
    class_weight_dict = dict(enumerate(weights))
    print(f"Class weights: {class_weight_dict}")
    
    # 5. Grid Search & Train
    best_model, best_params = run_grid_search(X, y, class_weight_dict)
    
    if best_model is None:
        print("Grid search failed to produce a model. Falling back to default.")
        best_model = build_model((X.shape[1], X.shape[2]), {})
        best_model.fit(X, y, epochs=50, batch_size=32, class_weight=class_weight_dict, verbose=1)

    # 6. Final Prediction
    y_pred = best_model.predict(X)
    
    # 7. Plot
    create_plot(full_df, dates, y, y_pred.flatten())
    
    # 8. Server
    run_server()

if __name__ == "__main__":
    main()
